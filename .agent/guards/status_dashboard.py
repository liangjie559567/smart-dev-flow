"""
status_dashboard.py â€” /status ä»ªè¡¨ç›˜ç”Ÿæˆå™¨ (T-308)

è¯»å– Axiom å„æ•°æ®æºï¼Œç”Ÿæˆç»“æ„åŒ– Markdown ä»ªè¡¨ç›˜ã€‚
å¯è¢«å·¥ä½œæµç›´æ¥è°ƒç”¨ï¼Œä¹Ÿå¯å‘½ä»¤è¡Œç‹¬ç«‹è¿è¡Œã€‚

Usage:
    from guards.status_dashboard import StatusDashboard
    dashboard = StatusDashboard(base_dir=".agent")
    print(dashboard.generate())
    
CLI:
    python .agent/guards/status_dashboard.py
"""

from __future__ import annotations

import os
import re
import sys
import subprocess
from collections import Counter
from datetime import datetime
from pathlib import Path
from typing import Any


class StatusDashboard:
    """ç³»ç»Ÿä»ªè¡¨ç›˜ç”Ÿæˆå™¨ã€‚"""

    def __init__(self, base_dir: str | Path = ".agent") -> None:
        self.base_dir = Path(base_dir)
        self.memory_dir = self.base_dir / "memory"
        self.evolution_dir = self.memory_dir / "evolution"
        self.config_dir = self.base_dir / "config"
        self.knowledge_dir = self.memory_dir / "knowledge"

    # â”€â”€ ä¸»å…¥å£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def generate(self) -> str:
        """ç”Ÿæˆå®Œæ•´ä»ªè¡¨ç›˜ Markdownã€‚"""
        sections = [
            self._header(),
            self._system_state(),
            self._task_progress(),
            self._evolution_stats(),
            self._recent_reflections(),
            self._workflow_metrics(),
            self._guard_status(),
            self._footer(),
        ]
        return "\n".join(sections)

    # â”€â”€ å„åŒºå—ç”Ÿæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _header(self) -> str:
        return "# ğŸ“Š Axiom â€” System Dashboard\n"

    def _system_state(self) -> str:
        """ç³»ç»ŸçŠ¶æ€åŒºå—ã€‚"""
        ctx = self._read_active_context()
        provider = self._read_active_provider()
        
        # è®¡ç®— uptime (è·ä¸Šæ¬¡ context æ›´æ–°)
        context_file = self.memory_dir / "active_context.md"
        uptime = "unknown"
        if context_file.exists():
            mtime = os.path.getmtime(context_file)
            age_min = int((datetime.now().timestamp() - mtime) / 60)
            uptime = f"{age_min} min"

        lines = [
            "## ğŸ¯ System State\n",
            "| Key | Value |",
            "|-----|-------|",
            f"| Status | `{ctx.get('task_status', 'UNKNOWN')}` |",
            f"| Session | `{ctx.get('session_id', 'N/A')}` |",
            f"| Provider | **{provider}** |",
            f"| Last Checkpoint | `{ctx.get('last_checkpoint', 'N/A')}` |",
            f"| Context Age | {uptime} since last update |",
            "",
            "---\n",
        ]
        return "\n".join(lines)

    def _task_progress(self) -> str:
        """ä»»åŠ¡è¿›åº¦åŒºå—ã€‚"""
        ctx_content = self._read_file(self.memory_dir / "active_context.md")
        
        # è§£æå„çŠ¶æ€ä»»åŠ¡ï¼Œå…¼å®¹ä¸¤ç§æ ¼å¼:
        # 1) [âœ… DONE] T-123
        # 2) - [x] **[DONE]** T-MW-001: ...
        done_tasks: list[str] = []
        pending_tasks: list[str] = []
        progress_tasks: list[str] = []
        blocked_tasks: list[str] = []
        failed_tasks: list[str] = []

        for line in ctx_content.splitlines():
            if "[DONE]" in line:
                bucket = done_tasks
            elif "[PENDING]" in line:
                bucket = pending_tasks
            elif "[IN_PROGRESS]" in line:
                bucket = progress_tasks
            elif "[BLOCKED]" in line:
                bucket = blocked_tasks
            elif "[FAILED]" in line:
                bucket = failed_tasks
            else:
                continue

            task_match = re.search(r"\b(T-[A-Za-z0-9-]+)\b", line)
            if task_match:
                bucket.append(task_match.group(1))
            else:
                bucket.append("-")
        
        total = len(done_tasks) + len(pending_tasks) + len(progress_tasks) + len(blocked_tasks) + len(failed_tasks)
        done_count = len(done_tasks)
        pct = int(done_count / total * 100) if total > 0 else 0
        
        # ç”Ÿæˆè¿›åº¦æ¡
        filled = pct // 5
        empty = 20 - filled
        bar = "â–ˆ" * filled + "â–‘" * empty

        lines = [
            "## ğŸ“‹ Task Progress\n",
            "| Status | Count | Tasks |",
            "|--------|-------|-------|",
            f"| âœ… Done | {len(done_tasks)} | {', '.join(done_tasks[:10]) or '-'} |",
            f"| â³ Pending | {len(pending_tasks)} | {', '.join(pending_tasks[:10]) or '-'} |",
            f"| ğŸ”„ In Progress | {len(progress_tasks)} | {', '.join(progress_tasks) or '-'} |",
            f"| ğŸš« Blocked | {len(blocked_tasks)} | {', '.join(blocked_tasks) or '-'} |",
            f"| âŒ Failed | {len(failed_tasks)} | {', '.join(failed_tasks) or '-'} |",
            "",
            f"**Overall**: {bar} {pct}% ({done_count}/{total} tasks)",
            "",
            "---\n",
        ]
        return "\n".join(lines)

    def _evolution_stats(self) -> str:
        """è¿›åŒ–å¼•æ“ç»Ÿè®¡åŒºå—ã€‚"""
        # çŸ¥è¯†åº“ç»Ÿè®¡
        kb_content = self._read_file(self.evolution_dir / "knowledge_base.md")
        knowledge_rows = re.findall(r"\|\s*k-\d+\s*\|", kb_content)
        knowledge_count = len(knowledge_rows)

        # åˆ†ç±»ç»Ÿè®¡
        categories = re.findall(r"\|\s*(architecture|debugging|pattern|workflow|tooling)\s*\|\s*(\d+)\s*\|", kb_content)
        cat_summary = " / ".join(f"{c} {n}" for n, c in categories) if categories else "N/A"

        # æ¨¡å¼åº“ç»Ÿè®¡
        pl_content = self._read_file(self.evolution_dir / "pattern_library.md")
        active_patterns = len(re.findall(r"Status:\s*ACTIVE", pl_content, re.IGNORECASE))
        candidate_patterns = len(re.findall(r"Status:\s*CANDIDATE", pl_content, re.IGNORECASE))

        # å­¦ä¹ é˜Ÿåˆ—
        lq_content = self._read_file(self.evolution_dir / "learning_queue.md")
        pending_items = len(re.findall(r"\|\s*pending\s*\|", lq_content, re.IGNORECASE))
        processed_items = len(re.findall(r"\|\s*processed\s*\|", lq_content, re.IGNORECASE))

        # åæ€æ¬¡æ•°
        rl_content = self._read_reflection_log()
        reflection_count = len(re.findall(r"^##\s+\d{4}-\d{2}-\d{2}\b", rl_content, re.MULTILINE))

        lines = [
            "## ğŸ§¬ Evolution Stats\n",
            "| Metric | Count | Details |",
            "|--------|-------|---------|",
            f"| ğŸ“š Knowledge Items | {knowledge_count} | {cat_summary} |",
            f"| ğŸ”„ Active Patterns | {active_patterns + candidate_patterns} | {active_patterns} ACTIVE / {candidate_patterns} CANDIDATE |",
            f"| ğŸ“¥ Learning Queue | {pending_items + processed_items} | {pending_items} pending / {processed_items} processed |",
            f"| ğŸ’­ Reflections | {reflection_count} | Last: {self._get_last_reflection_date(rl_content)} |",
            "",
            "---\n",
        ]
        return "\n".join(lines)

    def _recent_reflections(self) -> str:
        """æœ€è¿‘åæ€æ‘˜è¦åŒºå—ã€‚"""
        rl_content = self._read_reflection_log()
        
        # æå–åæ€ Session
        sessions = re.findall(r"##\s+(\d{4}-\d{2}-\d{2})\s+(.+?)(?:\n|$)", rl_content)

        lines = [
            "## ğŸ’­ Recent Reflections\n",
            "| Date | Session | Key Learning |",
            "|------|---------|-------------|",
        ]

        if sessions:
            for date, session_name in sessions[-5:]:
                # å°è¯•æå– Learnings éƒ¨åˆ†
                learning = self._extract_learning(rl_content, date)
                lines.append(f"| {date} | {session_name.strip()} | {learning} |")
        else:
            lines.append("| - | æš‚æ— åæ€è®°å½• | - |")

        lines.extend(["", "---\n"])
        return "\n".join(lines)

    def _workflow_metrics(self) -> str:
        """å·¥ä½œæµæŒ‡æ ‡è¶‹åŠ¿åŒºå—ã€‚"""
        wm_content = self._read_file(self.evolution_dir / "workflow_metrics.md")

        workflows = {
            "feature-flow": {"runs": 0, "avg_dur": "-", "success_rate": "-", "last_run": "-"},
            "analyze-error": {"runs": 0, "avg_dur": "-", "success_rate": "-", "last_run": "-"},
            "start": {"runs": 0, "avg_dur": "-", "success_rate": "-", "last_run": "-"},
        }

        for wf_name in workflows:
            rows = self._parse_workflow_rows(wm_content, wf_name)
            if rows:
                workflows[wf_name]["runs"] = len(rows)
                durations = [int(r[1]) for r in rows]
                successes = [r[2] == "âœ“" for r in rows]
                workflows[wf_name]["avg_dur"] = f"{sum(durations) // len(durations)}min"
                workflows[wf_name]["success_rate"] = f"{sum(successes) / len(successes) * 100:.0f}%"
                workflows[wf_name]["last_run"] = rows[-1][0]

        # å…¨å±€ç»Ÿè®¡
        global_match = re.search(r"æ€»æ‰§è¡Œæ¬¡æ•° \| (\d+)", wm_content)
        total_runs = global_match.group(1) if global_match else "0"

        lines = [
            "## ğŸ“ˆ Workflow Metrics\n",
            "| Workflow | Runs | Avg Duration | Success Rate | Last Run |",
            "|----------|------|-------------|-------------|----------|",
        ]
        for name, data in workflows.items():
            lines.append(
                f"| {name} | {data['runs']} | {data['avg_dur']} "
                f"| {data['success_rate']} | {data['last_run']} |"
            )
        lines.extend([
            "",
            f"**Total Workflow Executions**: {total_runs}",
            "",
            "---\n",
        ])
        return "\n".join(lines)

    def _guard_status(self) -> str:
        """å®ˆå«çŠ¶æ€åŒºå—ã€‚"""
        # æ£€æŸ¥ Git Hooks
        git_hooks_dir = self._find_git_hooks_dir()
        
        pre_commit = "âœ… Installed" if (git_hooks_dir / "pre-commit").exists() else "âŒ Not installed"
        post_commit = "âœ… Installed" if (git_hooks_dir / "post-commit").exists() else "âŒ Not installed"

        # æ£€æŸ¥æœ€è¿‘ checkpoint
        last_cp = self._get_last_checkpoint()

        # çœ‹é—¨ç‹—çŠ¶æ€ (æ£€æŸ¥è¿›ç¨‹)
        watchdog_status = "â¸ï¸ Stopped"

        lines = [
            "## ğŸ›¡ï¸ Guard Status\n",
            "| Guard | Status | Details |",
            "|-------|--------|---------|",
            f"| Pre-commit | {pre_commit} | Warning-only (ä¸é˜»æ–­) |",
            f"| Post-commit | {post_commit} | Auto-checkpoint (30min) |",
            f"| Session Watchdog | {watchdog_status} | `python .agent/guards/session_watchdog.py` |",
            f"| Last Checkpoint | {last_cp} | |",
            "",
            "---\n",
        ]
        return "\n".join(lines)

    def _footer(self) -> str:
        """é¡µè„šã€‚"""
        now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        return (
            f"*Dashboard generated at: {now}*\n"
            f"*Axiom v4.0*"
        )

    # â”€â”€ è¾…åŠ©æ–¹æ³• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _read_file(self, path: Path) -> str:
        """å®‰å…¨è¯»å–æ–‡ä»¶ã€‚"""
        try:
            return path.read_text(encoding="utf-8")
        except Exception:
            try:
                return path.read_bytes().decode("utf-8", errors="replace")
            except Exception:
                return ""

    def _read_reflection_log(self) -> str:
        """è¯»å–åæ€æ—¥å¿—ï¼Œå…¼å®¹æ—§/æ–°è·¯å¾„ã€‚"""
        candidates = [
            self.evolution_dir / "reflection_log.md",
            self.memory_dir / "reflection_log.md",
        ]
        for path in candidates:
            content = self._read_file(path)
            if content:
                return content
        return ""

    def _parse_workflow_rows(self, content: str, workflow: str) -> list[tuple[str, int, str]]:
        """è§£ææŒ‡å®š workflow çš„æ‰§è¡Œè®°å½•è¡Œã€‚"""
        section_map = {
            "feature-flow": "## 1. feature-flow",
            "analyze-error": "## 2. analyze-error",
            "start": "## 3. start",
        }
        marker = section_map.get(workflow)
        if not marker or marker not in content:
            return []

        rows: list[tuple[str, int, str]] = []
        in_section = False
        for line in content.splitlines():
            if line.strip() == marker:
                in_section = True
                continue
            if in_section and line.startswith("## "):
                break
            if not in_section:
                continue

            match = re.match(r"^\|\s*(\d{4}-\d{2}-\d{2})\s*\|\s*(\d+)min\s*\|\s*([âœ“âœ—])\s*\|", line)
            if match:
                date, duration, mark = match.groups()
                rows.append((date, int(duration), mark))

        return rows

    def _read_active_context(self) -> dict[str, str]:
        """è§£æ active_context.md çš„ YAML frontmatterã€‚"""
        content = self._read_file(self.memory_dir / "active_context.md")
        result: dict[str, str] = {}

        # æå– frontmatter
        fm_match = re.search(r"^---\s*\n(.*?\n)---", content, re.DOTALL)
        if fm_match:
            for line in fm_match.group(1).splitlines():
                kv = line.split(":", 1)
                if len(kv) == 2:
                    key = kv[0].strip()
                    val = kv[1].strip().strip('"')
                    result[key] = val

        return result

    def _read_active_provider(self) -> str:
        """è¯»å–å½“å‰æ¿€æ´»çš„ Providerã€‚"""
        content = self._read_file(self.config_dir / "agent_config.md")
        match = re.search(r"ACTIVE_PROVIDER:\s*(\w+)", content)
        return match.group(1) if match else "gemini"

    def _get_last_reflection_date(self, content: str) -> str:
        """è·å–æœ€åä¸€æ¬¡åæ€æ—¥æœŸã€‚"""
        dates = re.findall(r"^##\s+(\d{4}-\d{2}-\d{2})\b", content, re.MULTILINE)
        return dates[-1] if dates else "N/A"

    def _extract_learning(self, content: str, date: str) -> str:
        """ä»åæ€æ—¥å¿—ä¸­æå–æŸæ¬¡ Session çš„å…³é”® Learningã€‚"""
        # ç®€å•æå–è¯¥æ—¥æœŸåçš„ç¬¬ä¸€ä¸ª Learning è¡Œ
        pattern = rf"## {date} Session:.*?### ğŸ’¡ Learnings.*?\n- (.*?)(?:\n|$)"
        match = re.search(pattern, content, re.DOTALL)
        if match:
            return match.group(1).strip()[:60]
        return "-"

    def _find_git_hooks_dir(self) -> Path:
        """æŸ¥æ‰¾ .git/hooks ç›®å½•ã€‚"""
        current = Path.cwd()
        while current != current.parent:
            git_hooks = current / ".git" / "hooks"
            if git_hooks.is_dir():
                return git_hooks
            current = current.parent
        return Path(".git/hooks")

    def _get_last_checkpoint(self) -> str:
        """è·å–æœ€è¿‘çš„ checkpoint tagã€‚"""
        try:
            result = subprocess.run(
                ["git", "tag", "-l", "checkpoint-*", "--sort=-creatordate"],
                capture_output=True, text=True, timeout=5,
            )
            if result.returncode == 0 and result.stdout.strip():
                return result.stdout.strip().splitlines()[0]
        except Exception:
            pass
        return "N/A"


# â”€â”€ CLI å…¥å£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    # è‡ªåŠ¨æŸ¥æ‰¾ .agent ç›®å½•
    base = Path(".agent")
    if not base.exists():
        current = Path.cwd()
        while current != current.parent:
            candidate = current / ".agent"
            if candidate.is_dir():
                base = candidate
                break
            current = current.parent

    dashboard = StatusDashboard(base_dir=base)
    output = dashboard.generate()
    encoding = getattr(sys.stdout, "encoding", None) or "utf-8"
    stdout_buffer = getattr(sys.stdout, "buffer", None)
    if stdout_buffer is not None:
        stdout_buffer.write(output.encode(encoding, errors="backslashreplace") + b"\n")
    else:
        print(output.encode(encoding, errors="backslashreplace").decode(encoding, errors="ignore"))
